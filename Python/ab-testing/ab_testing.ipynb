{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Ultimate Guide To AB Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## AB testing helps us understand and quantify change "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we have a product or service that works in a certain way, we often want to make changes to improve this. When we make a change we want to be confident that the impact is \"real\" i.e. not caused by random chance. In order to be able to measure this change we can use AB testing.\n",
    "\n",
    "In an AB test we split the users into different groups; the control group stays with the same experience or product and then the other groups each receive a variation of this (often there is only one other variant). We then measure key metrics for each group and compare the results. AB testing can be used in many fields but is often used in product development.\n",
    "\n",
    "To make this more concrete for the rest of the article - imagine we are running a website and are looking to improve the sales conversion - i.e the proportion of users signing up who purchase on our website. We want to run an AB test to see if changing the styling of a key product page increases the conversion. The users are randomly split into two groups, we show the control group the old website and we show the users in the variant group the modified styling. We run the test for a period of time and observe how many from each group go on to purchase - we then calculate the statistics to determine whether the conversion rate has improved for the new style.\n",
    "\n",
    "You can use AB testing for other metrics such as average spend but the analysis is a little different.\n",
    "\n",
    "When calculating the statistics for an AB test there are two main schools of thought - `Freqentist` and `Bayesian`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Before we get into the detail let's define the number of users entering the test as $n$ and the number of users who convert as $s$ (I chose s for success).\n",
    "\n",
    "We can then split users these between the two variants. We use $n_C$ and $n_V$ to represent the number of users in the control and variant respectively. We use $s_C$ and $s_V$ to represent the number of users who convert in the control and variant respectively. Note $n = n_C + n_V$ and $s = s_C + s_V$.\n",
    "\n",
    "Now we can write the conversion rate $p$ as $p = \\frac{s}{n}$ and for control $p_C = \\frac{s_C}{n_C}$ and for the variant $p_V = \\frac{s_V}{n_V}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Frequentist Vs Bayesian thinking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Freqentist` and `Bayesian` approaches to testing differ in how they calculate test statistics but also in how they think about the problem. It is worth describing these two different ways of thinking about testing before going into the calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequentist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Frequentists` view the `true` theoretical conversion rate as a fixed unique value - for instance in our example it could be 10%. This underlying value is also called the `population conversion rate` but it can't be directly observed as we can't realistically have everyone in the world try out our website. Instead we observe the `sample conversion rate` from the sample of users we do observe. This may be slightly different from the underlying conversion rate due to the randomness in the sample.\n",
    "\n",
    "In this `Frequentist` setting we use known properties about samples to work out if the variant sample is `significantly` different from the control. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a `Bayesian` setting instead of considering the `population conversion rate` as a single unobservable value it is considered as a distribution. For instance it could be a distribution centered around 10% as shown below. This probability distribution expresses the idea that we think the conversion rate is itself random but most likely to be around 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "from scipy.stats import distributions\n",
    "\n",
    "def bayesian_example_distribution():\n",
    "    figure = go.FigureWidget()\n",
    "    \n",
    "    # Beta values to plot\n",
    "    n = 200\n",
    "    step = 1 / n\n",
    "    x = np.arange(0, 1, step)\n",
    "    beta = distributions.beta(a=10, b=100)\n",
    "    y = beta.pdf(x)\n",
    "\n",
    "    figure.add_scatter(x=x, y=y, fill='tozeroy', opacity=0.5)\n",
    "    \n",
    "    # labels\n",
    "    figure.layout.title = 'Example Conversion Rate Distribution'\n",
    "    figure.layout.xaxis.title = 'Conversion rate'\n",
    "    figure.layout.yaxis.title = 'Probability density'\n",
    "    figure.layout.xaxis.tickformat = '%'\n",
    "    figure.layout.yaxis.showticklabels = False\n",
    "\n",
    "    return figure\n",
    "\n",
    "example_distribution = bayesian_example_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c9cbc5333ec4c26b041a82b08ba48dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'fill': 'tozeroy',\n",
       "              'opacity': 0.5,\n",
       "              'type': 'scatter',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Test Significance - the idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we understand how `Freqentist` and `Bayesian` frameworks view conversion rates we can explain how the test statistics are calculated and what they mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequentist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Freqentist setting we use hypothesis testing - we assume there is no change in conversion between control and the variant groups then observe the results. We calculate the probability of observing a difference in conversion rates at least as large as we see (the p value) and if very unlikely we conclude the assumption was wrong and that the conversion rates must be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a frequentist setting we initially assume that both the control and variant groups have the same underlying conversion rate (we assume that they are sampled from the same population). This assumption or hypothesis is known as the `null hypothesis` denoted $H_0$.\n",
    "\n",
    "The `alternative hypothesis`, $H_1$ is just the opposite of the null hypothesis - that the underlying conversion rates are different.$^1$\n",
    "\n",
    "As we initially assume the underlying conversion rates are the same we expect the conversion rates of the control sample and variant sample to be close to each other and the underlying true value.\n",
    "\n",
    "Say if the underlying conversion rate was 10% (remember in practice we don't know this) then we would expect both the control and the variant to have roughly 10% of users convert under the null hypothesis.\n",
    "\n",
    "If the variant sample has an average conversion rate `significantly` different to the control we reject the null hypothesis and accept the alternative hypothesis.\n",
    "\n",
    "In order to decide whether the difference is significant we work out the $p$ value. The $p$ value is the probability of seeing a conversion rate in the variant at least as far away from the control conversion rate as we observed. If this $p$ is smaller than the preset significance value (also known as $\\alpha$ - often this is 5%) then we reject the null hypothesis. The $p$ value has corresponding `critical values` which are the conversion rates required to reject the null hypothesis (either much larger or much smaller than the control value)\n",
    "\n",
    "When sampling from a distribution where each user has a fixed probability of converting (the underlying conversion rate) then the conversion rate of a sample of users with size $n$ will tend to a normal distribution as we increase $n$. This is due to the central limit theorem and allows us to calculate the $p$ value.\n",
    "\n",
    "---\n",
    "*footnote*\n",
    "\n",
    "$^1$: Strictly speaking this formulation is the two tail variant - for a one tail hypothesis test the alternate hypothesis asserts that the variant conversion rate is greater than the control conversion rate or even vice-versa. However the one tail test is less common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A detailed look at the p value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bernoulli distribution is a discrete distribution having two possible outcomes (success and failure). The shorthand $X \\sim {\\rm Bernoulli}(p)$ is used to indicate that the random variable $X$ has the Bernoulli distribution with parameter $p$ - the chance of success, where $0 < p < 1$. We can write the probability mass function as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(X=0) &= 1-p \\\\\n",
    "P(X=1) &= p\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can model each website user as a Bernoulli random variable where they have the value 1 if they convert and 0 if they do not. Therefore we assume the control and the variant groups are both samples of random Bernoulli variables. Note that the conversion rate of each sample is the same as the mean of the random variables - both are equal to the proportion of success in the sample.\n",
    "\n",
    "In general if we have a distribution we can imagine taking many samples and then calculating the mean (or conversion rate) of each sample. These means would then have a distribution around the true mean - we call this distribution the sampling distribution of the mean. The central limit theorem tells us that the sampling distribution tends towards a normal distribution with a certain mean $\\mu$ and variance $\\sigma^2$.\n",
    "\n",
    "The mean can be estimated by the mean of the sample (i.e. the observed conversion rate). The central limit theorem tells us that the standard deviation $\\sigma_{\\bar{x}}$ of the sampling distribution of the mean (also know as the standard error) is equal to the population standard deviation $\\sigma$ divided by the square root of the size of the sample $n$.\n",
    "$$\n",
    "\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}}\n",
    "$$\n",
    "\n",
    "We don't know the population standard deviation but we can estimate it using the sample standard deviation. So we assume that the control sample conversion rate $\\mu(C,n_C)\\sim\\mathcal{N}(\\mu_C,\\sigma_C^2)$ where \n",
    "\n",
    "$$\n",
    "\\mu_C=c_C\\quad \\sigma_C = \\frac{\\text{s}_C}{\\sqrt{n_C}}\n",
    "$$\n",
    "\n",
    "Where $\\text{s}_C$ is the standard deviation of the control sample. Similarly we assume the variant conversion rate $\\mu(V,n_V)\\sim\\mathcal{N}(\\mu_V,\\sigma_V^2)$ where \n",
    "\n",
    "$$\n",
    "\\mu_V=c_V\\quad \\sigma_V = \\frac{\\text{s}_V}{\\sqrt{n_V}}\n",
    "$$\n",
    "Where $\\text{s}_V$ is the standard deviation of the variant sample.\n",
    "\n",
    "As both samples are made up of Bernoulli random variables the standard deviations are defined as\n",
    "\n",
    "$$\\text{s}_C = \\sqrt{c_C(1-c_C)}\\\\\\text{s}_V = \\sqrt{c_V(1-c_V)}$$\n",
    "\n",
    "\n",
    "\n",
    "We are interested in $D = \\mu(V,n_V) - \\mu(C,n_C)$ the difference between the two conversion rates. As we have assumed they are normally distributed we can say that the difference is also normally distributed. The Freqentist statistics all come from approximating the difference distribution as normal. \n",
    "\n",
    "$$\n",
    "D = \\mu(V,n_V) - \\mu(C,n_C) \\sim\\mathcal{N}(\\mu_V - \\mu_C, \\sigma_C^2+\\sigma_V^2) = \\mathcal{N}(\\mu_V -\\mu_C, \\frac{\\text{s}_C^2}{n_C} + \\frac{\\text{s}_V^2}{n_V})\n",
    "$$\n",
    "Note the variance of the difference distribution is equal to the sum of the variances:\n",
    "$$\n",
    "\\sigma_D = \\sigma_C^2+\\sigma_V^2 = \\frac{\\text{s}_C^2}{n_C}+\\frac{\\text{s}_V^2}{n_V}\n",
    "$$\n",
    "\n",
    "Under the null hypothesis $H_0$ we assume $\\mu_C=\\mu_V$ hence\n",
    "\n",
    "$$\n",
    "\\mu(V,n_V) - \\mu(C,n_C) \\sim\\mathcal{N}(0,\\frac{\\text{s}_C^2}{n_C}+\\frac{\\text{s}_V^2}{n_V})\n",
    "$$\n",
    "\n",
    "\n",
    "Specifically we want to know the p value, the probability that difference was at least as extreme as we observed.\n",
    "I.e. \n",
    "\n",
    "$$\n",
    "P(|\\mu(V,n_V) - \\mu(C,n_C)| > |c_V - c_C|) = 2 * P(Z > \\frac{|\\mu_V - \\mu_C|}{\\sqrt{\\frac{\\text{s}_C^2}{n_C}+\\frac{\\text{s}_V^2}{n_V}}}   )\n",
    "$$\n",
    "\n",
    "Where Z is the standard normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualising frequentist statistical significance with an example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example if we assume, the true underlying conversion rate is 20% and the control and variant groups both had 100 users. If we observed 21 conversions in control and 25 conversions the variant we may wonder whether this difference of 4% conversion is a significant result. \n",
    "\n",
    "Below we calculate the conversion rates and the variance of the control sampling distribution and the variant sampling distribution. We also approximate the variance of the difference distribution by summing the variances of the control and variant groups.\n",
    "\n",
    "Next we plot the difference distribution assuming $H_0$ (centered about 0 because the conversion rates are assume to be equal) and the difference distribution assuming $H_1$ (centered about the observed difference). The result is significant if the observed difference (the mean of the difference distribution under $H_1$) is to the right of the right critical value or to the left of the left critical value.\n",
    "\n",
    "In the example below the result is not significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_conversion_rate = 0.2\n",
    "\n",
    "control_users = 10000\n",
    "control_user_conversions = 2000\n",
    "\n",
    "variant_users = 10000\n",
    "variant_user_conversions = 2100\n",
    "\n",
    "control_conversion_rate = control_user_conversions / control_users\n",
    "variant_conversion_rate = variant_user_conversions / variant_users\n",
    "control_variance = (control_conversion_rate * (1 - control_conversion_rate)) \n",
    "variant_variance = (variant_conversion_rate * (1 - variant_conversion_rate))\n",
    "diffence_variance = (control_variance / control_users) + (variant_variance / variant_users)\n",
    "observed_difference = variant_conversion_rate - control_conversion_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b264e9112a76499187fab220ec55daa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'name': 'Null hypothesis difference distribution',\n",
       "              'type': 'scatter…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frequentist_difference(observed_difference=observed_difference,\n",
    "                       diffence_variance=diffence_variance,\n",
    "                       alpha=0.1,\n",
    "                       two_tale=True,\n",
    "                       show_alpha=True,\n",
    "                       show_power=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type I and Type II error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Frequentist testing a `type I` error is the rejection of a true null hypothesis (also known as a \"false positive\" finding or conclusion). This is equivalent to concluding the variant is significantly different when it really isn't - it just had a higher/lower conversion by chance. The type I error is controlled by the $\\alpha$ value. If the $\\alpha$ is set at 5% then in the long run we would expect a type I error 5% of the time. The type I error corresponds to the sample mean falling in the green tails in the plot above.\n",
    "\n",
    "A `type II` error is when we do not reject a false null hypothesis (also known as a \"false negative\"). This is equivalent to not concluding the variant is better even though it is. Type II error is sometimes denoted as $\\beta$. The power is defined as one minus the chance of type II error so ideally we want a large power value. The power value is related to the number of samples, the alpha value, the current conversion rate and the expected uplift (also know as the minimal detectable effect). Type II error is normally controlled by preselecting an expected uplift, calculating the required number of samples using power analysis and then running the test until there are enough samples.\n",
    "\n",
    "It is a very bad idea to calculate the p value before this point because repeated significance calculations can increase type I error. We will come back to this later in simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deriving the minimum sample size for Frequentist test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is best to first to visualise the power on a chart to then derive the relationship between sample size, minimal detectable effect, baseline conversion rate, alpha and power. The minimal detectable effect is the smallest uplift or difference you expect and want to measure.\n",
    "\n",
    "We want the variant conversion rate to be greater than the control conversion rate by the minimal detectable effect.\n",
    "\n",
    "The power is the probability of finding this minimal increase. As above we can plot the difference distributions assuming the mean difference is zero, i.e. under the null hypothesis and also assuming the mean difference is the minimal detectable effect, i.e. under the alternative hypothesis. On the chart the power is the probability under the alternative hypothesis difference distribution to the right of the critical value (the 95th percentile of the null difference distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequentist_difference(observed_difference=0.1,\n",
    "                       diffence_variance=diffence_variance,\n",
    "                       alpha=0.05,\n",
    "                       two_tale=True,\n",
    "                       show_alpha=True,\n",
    "                       show_power=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically we can write the following (where $d$ is the minimal detectable effect and $Z$ is the standard normal):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{power} &= P(D > \\sigma_D * Z_{\\frac{\\alpha}{2}}|D\\sim\\mathcal{N}(d, \\sigma_D))\\\\\n",
    "\\text{power} &= P\\left(\\frac{D - d}{\\sigma_D} > \\frac{\\sigma_D * Z_{\\frac{\\alpha}{2}} - d}{\\sigma_D}|\n",
    "    D\\sim\\mathcal{N}(d, \\sigma_D)\\right)\\\\\n",
    "\\text{power} &= P\\left(Z > Z_{\\frac{\\alpha}{2}} - \\frac{d}{\\sigma_D}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So we can define \n",
    "$$Z_{\\text{power}} = Z_{\\frac{\\alpha}{2}} - \\frac{d}{\\sigma_D}$$ \n",
    "\n",
    "But as\n",
    "\n",
    "$$\\beta = 1 - \\text{power}$$\n",
    "\n",
    "It follows \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Z_{\\beta} & = -Z_{\\text{power}}\\\\\n",
    "Z_{\\beta}  & = \\frac{d}{\\sigma_D} - Z_{\\frac{\\alpha}{2}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Rearranging\n",
    "\n",
    "$$\n",
    "\\frac{d}{\\sigma_D} = Z_{\\frac{\\alpha}{2}} + Z_{\\beta}\n",
    "$$\n",
    "\n",
    "Expanding $\\sigma_D$\n",
    "$$\n",
    "\\frac{d}{\\sqrt{\\frac{s_C^2}{n_C} + \\frac{s_V^2}{n_V}}} = Z_{\\frac{\\alpha}{2}} + Z_{\\beta}\n",
    "$$\n",
    "Assuming $n_C = n_V = n$ and rearranging for n we see\n",
    "$$\n",
    "n = \\sqrt{\\frac{\\left(Z_{\\frac{\\alpha}{2}} + Z_{\\beta}\\right)^2\\left(s_C^2+s_V^2\\right)}{d^2}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- describe the prior \n",
    "- describe the update process\n",
    "- plot the prior and the posterior\n",
    "- calculate the probability of beating control\n",
    "- explain the loss as a concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can possibly go wrong ... ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- repeated testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for ab-testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.special import betaln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input to test results\n",
    "\n",
    "# conversions\n",
    "control_users = 10000\n",
    "control_user_conversions = 2000\n",
    "variant_users = 10000\n",
    "variant_user_conversions = 2100\n",
    "\n",
    "# settings\n",
    "alpha = 0.05\n",
    "beta = 0.2\n",
    "power = 1 - beta\n",
    "# bayesian prior settings\n",
    "prior_alpha = 1\n",
    "prior_beta = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calulations\n",
    "\n",
    "# Frequentist\n",
    "control_conversion_rate = control_user_conversions / control_users\n",
    "variant_conversion_rate = variant_user_conversions / variant_users\n",
    "observed_difference = variant_conversion_rate - control_conversion_rate\n",
    "control_variance = control_conversion_rate * (1 - control_conversion_rate)\n",
    "variant_variance = variant_conversion_rate * (1 - variant_conversion_rate)\n",
    "p_hat = (control_user_conversions + variant_user_conversions) / (control_users + variant_users)\n",
    "pooled_variance = (p_hat * (1 - p_hat)) * ((1 / control_users) + (1 / variant_users))\n",
    "# very similar in practise:\n",
    "# satterthwaite_variance = (control_variance / control_users) + (variant_variance / variant_users)\n",
    "pooled_standard_deviation = pooled_variance ** 0.5\n",
    "\n",
    "# Assuming two tail test (hence the * 2 on p_value)\n",
    "z_value = observed_difference / pooled_standard_deviation\n",
    "p_value = norm.sf(abs(z_value)) * 2\n",
    "frequentist_significant = p_value < alpha\n",
    "\n",
    "\n",
    "\n",
    "# Bayesian\n",
    "control_posterior_alpha = prior_alpha + control_user_conversions\n",
    "control_posterior_beta = prior_beta + control_users - control_user_conversions\n",
    "variant_posterior_alpha = prior_alpha + variant_user_conversions\n",
    "variant_posterior_beta = prior_beta + variant_users - variant_user_conversions\n",
    "n_beta_samples = 1_000_000\n",
    "diff_distribution = (\n",
    "    np.random.beta(variant_posterior_alpha, variant_posterior_beta, n_beta_samples) - \n",
    "    np.random.beta(control_posterior_alpha, control_posterior_beta, n_beta_samples))\n",
    "chance_to_beat_control = (diff_distribution > 0).sum() / diff_distribution.shape[0]\n",
    "# # Could calculate directly using the following line but then harder to inspect quantiles\n",
    "# chance_to_beat_control = prob_beat_control_faster(\n",
    "#     control_posterior_alpha, control_posterior_beta,\n",
    "#     variant_posterior_alpha, variant_posterior_beta)\n",
    "# Assuming Two tail - not really a Bayesian concept\n",
    "bayesian_significant = chance_to_beat_control < (alpha / 2) or chance_to_beat_control > (1 - (alpha / 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07984985065721206"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.960067"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chance_to_beat_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequentist_significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayesian_significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful ab test functions\n",
    "def prob_beat_control_sample(alpha_c, beta_c, alpha_v, beta_v,\n",
    "                             n=10_000, random_state=101):\n",
    "    \"\"\"Calculate chance to beat control using sampling\n",
    "\n",
    "    Speed:\n",
    "        n=10_000 => 2.43 ms\n",
    "        n=100_000 => 21.8 ms\n",
    "        n=1_000_000 => 226 ms\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    diff_distribution = (np.random.beta(alpha_v, beta_v, n) -\n",
    "                         np.random.beta(alpha_c, beta_c, n))\n",
    "    return (diff_distribution > 0).sum() / diff_distribution.shape[0]\n",
    "\n",
    "\n",
    "def prob_beat_control_faster(alpha_c, beta_c, alpha_v, beta_v):\n",
    "    \"\"\"Calculate chance to beat control using sampling\n",
    "\n",
    "    Speed:\n",
    "        min(alpha_v, beta_v) = 1_000 => 192 µs\n",
    "        min(alpha_v, beta_v) = 10_000 => 1.22 ms\n",
    "        min(alpha_v, beta_v) = 100_000 => 11.3 ms\n",
    "    \"\"\"\n",
    "    if alpha_v < beta_v:\n",
    "        i_s = np.arange(alpha_v)\n",
    "        prob_beat_control = np.exp(\n",
    "            betaln(alpha_c + i_s, beta_v + beta_c) -\n",
    "            np.log(beta_v + i_s) -\n",
    "            betaln(1 + i_s, beta_v) -\n",
    "            betaln(alpha_c, beta_c)).sum()\n",
    "    else:\n",
    "        i_s = np.arange(beta_v)\n",
    "        prob_beat_control = 1 - np.exp(\n",
    "            betaln(beta_c + i_s, alpha_c + alpha_v) -\n",
    "            np.log(alpha_v + i_s) -\n",
    "            betaln(1 + i_s, alpha_v) -\n",
    "            betaln(alpha_c, beta_c)).sum()\n",
    "    return np.clip(prob_beat_control, 0, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "from scipy.stats import norm\n",
    "from ipywidgets import widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequentist_difference(observed_difference,\n",
    "                           diffence_variance,\n",
    "                           alpha=0.05,\n",
    "                           two_tale=True,\n",
    "                           show_alpha=True,\n",
    "                           show_power=False):\n",
    "    null_difference_dist = norm(loc=0, scale=diffence_variance ** 0.5)\n",
    "    alt_difference_dist = norm(loc=observed_difference, scale=diffence_variance ** 0.5)\n",
    "\n",
    "    start_prob = 0.0001\n",
    "    end_prob = 1 - start_prob\n",
    "    null_start, null_end  = null_difference_dist.ppf([start_prob, end_prob])\n",
    "    alt_start, alt_end = alt_difference_dist.ppf([start_prob, end_prob])\n",
    "    start = min(null_start, alt_start)\n",
    "    end = max(null_end, alt_end)\n",
    "    \n",
    "    n_points = 500\n",
    "    step = (end - start) / n_points\n",
    "    plot_conversion_rates = np.arange(start, end, step)\n",
    "    null_plot_conversion_rates_probs = null_difference_dist.pdf(plot_conversion_rates)\n",
    "    alt_plot_conversion_rates_probs = alt_difference_dist.pdf(plot_conversion_rates)\n",
    "\n",
    "    fig = go.FigureWidget()\n",
    "    fig.add_scatter(\n",
    "        x=plot_conversion_rates,\n",
    "        y=null_plot_conversion_rates_probs,\n",
    "        name=\"Null hypothesis difference distribution\")\n",
    "#     fig.add_scatter(\n",
    "#         x=plot_conversion_rates,\n",
    "#         y=alt_plot_conversion_rates_probs,\n",
    "#         name=\"Alternative hypothesis difference distribution\")\n",
    "    fig.layout.title = 'Difference distributions'\n",
    "    fig.layout.xaxis.title = 'Difference d'\n",
    "    fig.layout.yaxis.title = 'Probability density'\n",
    "\n",
    "    if two_tale:\n",
    "            alpha = alpha / 2\n",
    "            \n",
    "    if show_alpha:\n",
    "        plot_alpha_right_x = np.arange(null_difference_dist.ppf(1 - alpha), end, step)\n",
    "        plot_alpha_right_y = null_difference_dist.pdf(plot_alpha_right_x)\n",
    "        plot_alpha_left_x = np.arange(start, null_difference_dist.ppf(alpha), step)\n",
    "        plot_alpha_left_y = null_difference_dist.pdf(plot_alpha_left_x)\n",
    "        right = fig.add_scatter(\n",
    "            x=plot_alpha_right_x,\n",
    "            y=plot_alpha_right_y,\n",
    "            line=dict(color=\"green\"),\n",
    "            fill='tozeroy',\n",
    "            name=f\"{alpha:.2%} prob\")\n",
    "        left = fig.add_scatter(\n",
    "            x=plot_alpha_left_x,\n",
    "            y=plot_alpha_left_y,\n",
    "            fill='tozeroy',\n",
    "            line=dict(color=\"green\"),\n",
    "            name=f\"{alpha:.2%} prob\",\n",
    "            visible=two_tale)\n",
    "\n",
    "    if show_power:\n",
    "        plot_power_right_x = np.arange(null_difference_dist.ppf(1 - alpha), end, step)\n",
    "        plot_power_right_y = alt_difference_dist.pdf(plot_power_right_x)\n",
    "        power = 1 - alt_difference_dist.cdf(null_difference_dist.ppf(1 - alpha))\n",
    "        power_line = fig.add_scatter(\n",
    "            x=plot_power_right_x,\n",
    "            y=plot_power_right_y,\n",
    "            line=dict(color=\"orange\"),\n",
    "            fill='tozeroy',\n",
    "            name=f\"Power: {power:.2%}\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'go' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-644be882ca5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfigure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFigureWidget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Beta values to plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'go' is not defined"
     ]
    }
   ],
   "source": [
    "figure = go.FigureWidget()\n",
    "    \n",
    "# Beta values to plot\n",
    "n = 200\n",
    "step = 1 / n\n",
    "x = np.arange(0, 1, step)\n",
    "\n",
    "\n",
    "for a, b in [(2,8), (20,80),(200, 800),(2000, 8000)]:\n",
    "    beta = distributions.beta(a=a, b=b)\n",
    "    y = beta.pdf(x)\n",
    "    figure.add_scatter(x=x, y=y, fill='tozeroy',opacity=0.5,\n",
    "                      name=f'Belief after seeing {a+b} users and {a} conversions ')\n",
    "    \n",
    "\n",
    "# labels\n",
    "figure.layout.title = 'Example Conversion Rate Distribution'\n",
    "figure.layout.xaxis.title = 'Conversion rate'\n",
    "figure.layout.yaxis.title = 'Probability density'\n",
    "figure.layout.xaxis.tickformat = '%'\n",
    "figure.layout.yaxis.showticklabels = False\n",
    "figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "494.062px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
